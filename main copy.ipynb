{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A sentiment analysis on tweets related to E-levy.\n",
    "\n",
    "#### This project seems to analyze the general consensus on the introduction of e-levy (electronic levy) charges by the Ghana Government on digital transactions. \n",
    "\n",
    "It uses the ------ NLP model to analyze the sentiments of hundreds of tweets scraped from twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing first libraries <br>\n",
    "<ul>\n",
    "<li>snscrape ---> a scraper for social networking services </li>\n",
    "<li>pandas ---> an open source data analysis and manipulation tool </li> \n",
    "<li>re ---> library for string manipulation</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Innocent\n",
      "[nltk_data]     Anyaele\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Innocent\n",
      "[nltk_data]     Anyaele\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "import preprocessor as p\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we build our query here by gathering e-levy related tweets from when it was first announced (2022/09/20)\n",
    "\n",
    "we store our gathered tweets in a csv file -> streams.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeTweets():    \n",
    "    query = '\"e-levy\" lang:en until:2022-09-20 since:2022-05-01 -filter:links'\n",
    "\n",
    "    tweets = []\n",
    "    limit = 500\n",
    "\n",
    "    data = sntwitter.TwitterSearchScraper(query).get_items()\n",
    "    for tweet in data:\n",
    "        if len(tweets) == limit:\n",
    "            break\n",
    "        else:\n",
    "            tweet_text = tweet.content\n",
    "            tweets.append([tweet.id, tweet_text, tweet.date])\n",
    "            \n",
    "    df = pd.DataFrame(tweets, columns=['id', 'Tweet', 'Date'])\n",
    "\n",
    "    df.to_csv('stream.csv', index=False, columns=['id','Tweet','Date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tweet containes 500 rows, each row containing the tweet id, tweet content and the tweet date.\n",
    "We print the first 5 tweets\n",
    "We see our tweets, contains a lot of unneccessary data for analysis, so we preprocess it in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 3)\n",
      "0    @edburtler @shamimamuslim Notice that Shamima ...\n",
      "1    @edburtler @shamimamuslim QUESTION: \\nWhy was ...\n",
      "2    @FrankOw18664478 @hearttooclean @mandemthe1st ...\n",
      "3    @FrankOw18664478 @hearttooclean @mandemthe1st ...\n",
      "4    @FrankOw18664478 @bra_Kofi__ @hearttooclean @m...\n",
      "Name: Tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('stream.csv')\n",
    "print(df.shape)\n",
    "print(df['Tweet'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing stage involves cleaning up our tweets with a regex function by removing links, tags and whitespaces.\n",
    "\n",
    "We also leverage the NLTK library to remove stop words such as \"and\", \"or\", \"in\"\n",
    "\n",
    "After we print some of our tweets, it looks much more cleaner than before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495    minister communication ursulaow mocghana moigo...\n",
      "496    georgekankambo4 hmmm tear even cedis elevy npp...\n",
      "497    vodafoneghana charged elevy transaction less c...\n",
      "498                            citi973 like refund elevy\n",
      "499                    everytime pay elevy dey feel sick\n",
      "Name: Tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def tweet_preprocessing(tweet):\n",
    "    # regex cleanup\n",
    "    tweet = re.sub(r\"^https://t.co/[A-Za-z0-9]*\\s\", \" \", tweet)\n",
    "    tweet = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*\\s\", \" \", tweet)\n",
    "    tweet = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*$\", \" \", tweet)\n",
    "    tweet = re.sub(\"\\.\\.+\", \" \", tweet)\n",
    "    tweet = re.sub(\"-$\", \"\", tweet)\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "    tweet = re.sub(r\"^ +\", \"\", tweet)\n",
    "    tweet = re.sub(r\"  +\", \" \", tweet)\n",
    "    \n",
    "    # use preprocessing library to clean\n",
    "    tweet = p.clean(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # tokenize\n",
    "    token_tweet = word_tokenize(tweet)\n",
    "    filtered = [w for w in token_tweet if not w.lower() in stop_words]\n",
    "    filtered_array  = []\n",
    "    \n",
    "    # remove stopwords\n",
    "    for w in token_tweet:\n",
    "        if w not in stop_words:\n",
    "            filtered_array.append(w)\n",
    "                 \n",
    "    \n",
    "    return ' '.join(filtered_array)\n",
    "\n",
    "\n",
    "# applying our pre processing function to our tweet\n",
    "df['Tweet'] = df['Tweet'].apply(lambda x: tweet_preprocessing(x))\n",
    "print (df['Tweet'].tail(5)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0eed52651247649b56a308431bf489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1571994188660744192: {'negative': 0.33652392, 'neutral': 0.20414186, 'positive': 0.1756662}, 1571992159183593473: {'negative': 0.22968423, 'neutral': 0.21303397, 'positive': 0.22564976}, 1571981363699679233: {'negative': 0.17080615, 'neutral': 0.15704201, 'positive': 0.21783318}, 1571981131238543366: {'negative': 0.17584077, 'neutral': 0.12185262, 'positive': 0.15068156}, 1571980917006278656: {'negative': 0.2278475, 'neutral': 0.13903745, 'positive': 0.15184657}, 1571980514512232452: {'negative': 0.4672054, 'neutral': 0.14724894, 'positive': 0.14081949}, 1571960199564460033: {'negative': 0.65381294, 'neutral': 0.17703915, 'positive': 0.10964892}, 1571958217692909568: {'negative': 0.29376715, 'neutral': 0.15764081, 'positive': 0.14446248}, 1571955389612322816: {'negative': 0.76084423, 'neutral': 0.15583694, 'positive': 0.058712028}, 1571955188034056192: {'negative': 0.4105462, 'neutral': 0.2875442, 'positive': 0.18929048}, 1571952217560875010: {'negative': 0.2579873, 'neutral': 0.1335384, 'positive': 0.15142158}, 1571948263447662592: {'negative': 0.5379015, 'neutral': 0.34273, 'positive': 0.10022537}, 1571946836507725825: {'negative': 0.63980293, 'neutral': 0.15049973, 'positive': 0.081084065}, 1571945066461073408: {'negative': 0.56182724, 'neutral': 0.20391594, 'positive': 0.12704012}, 1571941059378257926: {'negative': 0.47410998, 'neutral': 0.22614634, 'positive': 0.1614844}, 1571936146082435072: {'negative': 0.5938623, 'neutral': 0.18075842, 'positive': 0.09759852}, 1571935813574819840: {'negative': 0.15061855, 'neutral': 0.28910238, 'positive': 0.39209726}, 1571935739381686273: {'negative': 0.49569035, 'neutral': 0.29208913, 'positive': 0.15080494}, 1571933485899849730: {'negative': 0.13201661, 'neutral': 0.14378707, 'positive': 0.19269204}, 1571933337325023233: {'negative': 0.33620885, 'neutral': 0.25686023, 'positive': 0.20153077}, 1571919899462717441: {'negative': 0.6975404, 'neutral': 0.17690796, 'positive': 0.08485182}, 1571911517737746437: {'negative': 0.26819712, 'neutral': 0.1730266, 'positive': 0.16379586}, 1571910671545016321: {'negative': 0.20025255, 'neutral': 0.13165104, 'positive': 0.14762616}, 1571910067082993671: {'negative': 0.52996176, 'neutral': 0.19547492, 'positive': 0.14331709}, 1571909689725841408: {'negative': 0.0772965, 'neutral': 0.06911587, 'positive': 0.14602673}, 1571908599458177026: {'negative': 0.47950375, 'neutral': 0.19215095, 'positive': 0.14549674}, 1571906973809127424: {'negative': 0.46060795, 'neutral': 0.25619182, 'positive': 0.14655048}, 1571893854822957061: {'negative': 0.3986578, 'neutral': 0.22072992, 'positive': 0.15931918}, 1571893206630940672: {'negative': 0.32492632, 'neutral': 0.3621034, 'positive': 0.22149077}, 1571892940313616384: {'negative': 0.7551526, 'neutral': 0.13853616, 'positive': 0.0577593}, 1571891383795064833: {'negative': 0.6512762, 'neutral': 0.18096887, 'positive': 0.10433753}, 1571890460863479812: {'negative': 0.031440523, 'neutral': 0.028075688, 'positive': 0.08682882}, 1571890376004489222: {'negative': 0.24926853, 'neutral': 0.17132625, 'positive': 0.26369545}, 1571890308945743872: {'negative': 0.42910516, 'neutral': 0.24369355, 'positive': 0.20795336}, 1571888519836209152: {'negative': 0.5421773, 'neutral': 0.21072727, 'positive': 0.13794453}, 1571885624021454856: {'negative': 0.54269564, 'neutral': 0.2320041, 'positive': 0.1115053}, 1571882650448830464: {'negative': 0.66150665, 'neutral': 0.178049, 'positive': 0.09666686}, 1571882427647430659: {'negative': 0.6244046, 'neutral': 0.22430126, 'positive': 0.09193699}, 1571879675164790791: {'negative': 0.029605562, 'neutral': 0.039231416, 'positive': 0.07540746}, 1571879571728875521: {'negative': 0.69352, 'neutral': 0.20043728, 'positive': 0.0689623}, 1571869129677709316: {'negative': 0.57452434, 'neutral': 0.17847367, 'positive': 0.12179368}, 1571868349855944704: {'negative': 0.62914467, 'neutral': 0.1885857, 'positive': 0.06552177}, 1571866993006972928: {'negative': 0.2859307, 'neutral': 0.124599315, 'positive': 0.11634549}, 1571866839734693888: {'negative': 0.70249456, 'neutral': 0.2193462, 'positive': 0.054580145}, 1571866806486278145: {'negative': 0.7106275, 'neutral': 0.18388978, 'positive': 0.06613575}, 1571862092705157120: {'negative': 0.52777046, 'neutral': 0.33320686, 'positive': 0.10939884}, 1571858811148800000: {'negative': 0.70703655, 'neutral': 0.13253717, 'positive': 0.076623976}, 1571852144789299200: {'negative': 0.1706274, 'neutral': 0.08520866, 'positive': 0.11472986}, 1571840155002048513: {'negative': 0.25205058, 'neutral': 0.1916308, 'positive': 0.25239936}, 1571838662115037184: {'negative': 0.40674892, 'neutral': 0.26309544, 'positive': 0.17019592}, 1571836101677158400: {'negative': 0.27479967, 'neutral': 0.29641774, 'positive': 0.22154869}, 1571834309220122626: {'negative': 0.16451305, 'neutral': 0.1687235, 'positive': 0.24410729}, 1571834236390326273: {'negative': 0.18348856, 'neutral': 0.25710803, 'positive': 0.33113676}, 1571832877687644162: {'negative': 0.19983087, 'neutral': 0.33414266, 'positive': 0.318715}, 1571830079562432512: {'negative': 0.03917415, 'neutral': 0.02618058, 'positive': 0.05378401}, 1571828148869767175: {'negative': 0.4406409, 'neutral': 0.17754118, 'positive': 0.15414639}, 1571826604241784833: {'negative': 0.6097489, 'neutral': 0.2099506, 'positive': 0.108382165}, 1571826397919715334: {'negative': 0.618688, 'neutral': 0.23781691, 'positive': 0.11807638}, 1571826203547373569: {'negative': 0.1680396, 'neutral': 0.10366037, 'positive': 0.1708984}, 1571823875733815296: {'negative': 0.41214335, 'neutral': 0.23999475, 'positive': 0.1688124}, 1571823577036464130: {'negative': 0.11849404, 'neutral': 0.10619216, 'positive': 0.1307241}, 1571818953898352640: {'negative': 0.8350605, 'neutral': 0.11477528, 'positive': 0.039503448}, 1571818844909441025: {'negative': 0.20342116, 'neutral': 0.12724043, 'positive': 0.1737242}, 1571818043860258816: {'negative': 0.07470452, 'neutral': 0.09461044, 'positive': 0.19917434}, 1571816398250446851: {'negative': 0.56643313, 'neutral': 0.2579335, 'positive': 0.12918133}, 1571815912285655043: {'negative': 0.7043974, 'neutral': 0.1857495, 'positive': 0.07199498}, 1571814810035027968: {'negative': 0.4879646, 'neutral': 0.18952784, 'positive': 0.14812468}, 1571812797742202882: {'negative': 0.1438478, 'neutral': 0.12852137, 'positive': 0.21169719}, 1571812037654724609: {'negative': 0.6621669, 'neutral': 0.2249601, 'positive': 0.090492114}, 1571797216225804288: {'negative': 0.44729382, 'neutral': 0.24624674, 'positive': 0.15809676}, 1571794564284923905: {'negative': 0.6581832, 'neutral': 0.056527972, 'positive': 0.027998421}, 1571780545931083777: {'negative': 0.5126556, 'neutral': 0.18310055, 'positive': 0.13956365}, 1571779982921285633: {'negative': 0.2510971, 'neutral': 0.14131555, 'positive': 0.17888044}, 1571779962079698944: {'negative': 0.4879646, 'neutral': 0.18952784, 'positive': 0.14812468}, 1571777649630117888: {'negative': 0.32021838, 'neutral': 0.23212467, 'positive': 0.16660488}, 1571772419064598530: {'negative': 0.22272742, 'neutral': 0.3788007, 'positive': 0.2962791}, 1571772157012725760: {'negative': 0.32756978, 'neutral': 0.3742629, 'positive': 0.22431739}, 1571771244026138625: {'negative': 0.5645362, 'neutral': 0.2532225, 'positive': 0.11079805}, 1571769698080915461: {'negative': 0.3562384, 'neutral': 0.21775265, 'positive': 0.14029555}, 1571766947469201408: {'negative': 0.24287233, 'neutral': 0.37324363, 'positive': 0.233989}, 1571765643535618049: {'negative': 0.17632283, 'neutral': 0.19805875, 'positive': 0.27960324}, 1571764543667277825: {'negative': 0.47106466, 'neutral': 0.28647998, 'positive': 0.12137741}, 1571762846807085057: {'negative': 0.3967415, 'neutral': 0.3284236, 'positive': 0.19403714}, 1571749714059460610: {'negative': 0.04193475, 'neutral': 0.038588427, 'positive': 0.091821946}, 1571634176897552390: {'negative': 0.4386155, 'neutral': 0.19520986, 'positive': 0.15606712}, 1571633287168401410: {'negative': 0.4686722, 'neutral': 0.18729576, 'positive': 0.16457342}, 1571629840645726213: {'negative': 0.3162534, 'neutral': 0.29698864, 'positive': 0.24168897}, 1571620742617305089: {'negative': 0.31484583, 'neutral': 0.23481625, 'positive': 0.22630784}, 1571590235120713735: {'negative': 0.099409096, 'neutral': 0.06803703, 'positive': 0.15627442}, 1571586130432217088: {'negative': 0.6031051, 'neutral': 0.21632566, 'positive': 0.12924957}, 1571569233754001409: {'negative': 0.6826391, 'neutral': 0.12175826, 'positive': 0.06099543}, 1571535602478112769: {'negative': 0.5316331, 'neutral': 0.26549396, 'positive': 0.11655962}, 1571534637771407363: {'negative': 0.06824917, 'neutral': 0.05703126, 'positive': 0.10408841}, 1571533446165499905: {'negative': 0.31178886, 'neutral': 0.23021352, 'positive': 0.21912542}, 1571525910972481536: {'negative': 0.542299, 'neutral': 0.23924306, 'positive': 0.10964979}, 1571510724848934915: {'negative': 0.46742874, 'neutral': 0.2544145, 'positive': 0.15462184}, 1571510199264985088: {'negative': 0.55676866, 'neutral': 0.23655842, 'positive': 0.13605554}, 1571509964140843008: {'negative': 0.5954043, 'neutral': 0.24460945, 'positive': 0.099181585}, 1571509203214434307: {'negative': 0.40140507, 'neutral': 0.26281464, 'positive': 0.18332376}, 1571500062890663941: {'negative': 0.023224805, 'neutral': 0.035083387, 'positive': 0.10317081}}\n"
     ]
    }
   ],
   "source": [
    "def polarity_scores_roberta(example):\n",
    "    encoded_text = tokenizer(example, return_tensors='pt')\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    scores_dict = {\n",
    "        'negative': scores[0],\n",
    "        'neutral': scores[1],\n",
    "        'positive': scores[2]\n",
    "    }\n",
    "    return scores_dict\n",
    "\n",
    "\n",
    "res = {}\n",
    "for i, row in tqdm(twitter_df.iterrows(), total=len(df)):\n",
    "    try:   \n",
    "        tweet_text = row['Tweet']\n",
    "        tweet_id = row['id']\n",
    "        roberta_results = polarity_scores_roberta(tweet_text)\n",
    "        res[tweet_id] = roberta_results\n",
    "    except RuntimeError:\n",
    "        tweet_id = row['id']\n",
    "        print (\"Broke for {tweet_id} \")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     id                                              Tweet  \\\n",
      "0   1571994188660744192  @edburtler @shamimamuslim Notice that Shamima ...   \n",
      "1   1571992159183593473  @edburtler @shamimamuslim QUESTION: \\nWhy was ...   \n",
      "2   1571981363699679233  @FrankOw18664478 @hearttooclean @mandemthe1st ...   \n",
      "3   1571981131238543366  @FrankOw18664478 @hearttooclean @mandemthe1st ...   \n",
      "4   1571980917006278656  @FrankOw18664478 @bra_Kofi__ @hearttooclean @m...   \n",
      "..                  ...                                                ...   \n",
      "95  1571510724848934915  @Mandemthe1st @ShoeLhaze @BongoIdeas I should ...   \n",
      "96  1571510199264985088  @GhanaRevenue why are the Telcos charging e-le...   \n",
      "97  1571509964140843008  @ShoeLhaze @Mandemthe1st @BongoIdeas Bro this ...   \n",
      "98  1571509203214434307  @Mandemthe1st @ShoeLhaze @BongoIdeas Is Afro d...   \n",
      "99  1571500062890663941  @Mandemthe1st @ShoeLhaze @BongoIdeas Lol 😂 😹 y...   \n",
      "\n",
      "                        Date  negative   neutral  positive  \n",
      "0  2022-09-19 22:46:35+00:00  0.020204  0.390654  0.589141  \n",
      "1  2022-09-19 22:38:31+00:00  0.020204  0.390654  0.589141  \n",
      "2  2022-09-19 21:55:37+00:00  0.020204  0.390654  0.589141  \n",
      "3  2022-09-19 21:54:42+00:00  0.020204  0.390654  0.589141  \n",
      "4  2022-09-19 21:53:51+00:00  0.020204  0.390654  0.589141  \n",
      "..                       ...       ...       ...       ...  \n",
      "95 2022-09-18 14:45:28+00:00  0.020204  0.390654  0.589141  \n",
      "96 2022-09-18 14:43:23+00:00  0.020204  0.390654  0.589141  \n",
      "97 2022-09-18 14:42:27+00:00  0.020204  0.390654  0.589141  \n",
      "98 2022-09-18 14:39:25+00:00  0.020204  0.390654  0.589141  \n",
      "99 2022-09-18 14:03:06+00:00  0.020204  0.390654  0.589141  \n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(res).T\n",
    "results_df = results_df.reset_index().rename(columns={'index' : 'id'})\n",
    "merged_df = pd.merge(twitter_df,results_df,how='outer')\n",
    "print (merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 39.0/39.0 [00:00<00:00, 18.7kB/s]\n",
      "Downloading: 100%|██████████| 953/953 [00:00<00:00, 144kB/s]\n",
      "Downloading: 100%|██████████| 851k/851k [00:18<00:00, 46.3kB/s] \n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 113kB/s]\n",
      "Downloading: 100%|██████████| 638M/638M [20:59<00:00, 531kB/s]    \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sentiment_score(tweet):\n",
    "    tokens = tokenizer.encode(tweet, return_tensors='pt')\n",
    "    result = model(tokens)\n",
    "    result.logits\n",
    "    return (int(torch.argmax(result.logits))+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = twitter_df\n",
    "data1['sentiment_score'] = data1['Tweet'].apply(lambda x: sentiment_score(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Date</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1571994188660744192</td>\n",
       "      <td>@edburtler @shamimamuslim Notice that Shamima ...</td>\n",
       "      <td>2022-09-19 22:46:35+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1571992159183593473</td>\n",
       "      <td>@edburtler @shamimamuslim QUESTION: \\nWhy was ...</td>\n",
       "      <td>2022-09-19 22:38:31+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1571981363699679233</td>\n",
       "      <td>@FrankOw18664478 @hearttooclean @mandemthe1st ...</td>\n",
       "      <td>2022-09-19 21:55:37+00:00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1571981131238543366</td>\n",
       "      <td>@FrankOw18664478 @hearttooclean @mandemthe1st ...</td>\n",
       "      <td>2022-09-19 21:54:42+00:00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1571980917006278656</td>\n",
       "      <td>@FrankOw18664478 @bra_Kofi__ @hearttooclean @m...</td>\n",
       "      <td>2022-09-19 21:53:51+00:00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              Tweet  \\\n",
       "0  1571994188660744192  @edburtler @shamimamuslim Notice that Shamima ...   \n",
       "1  1571992159183593473  @edburtler @shamimamuslim QUESTION: \\nWhy was ...   \n",
       "2  1571981363699679233  @FrankOw18664478 @hearttooclean @mandemthe1st ...   \n",
       "3  1571981131238543366  @FrankOw18664478 @hearttooclean @mandemthe1st ...   \n",
       "4  1571980917006278656  @FrankOw18664478 @bra_Kofi__ @hearttooclean @m...   \n",
       "\n",
       "                       Date  sentiment_score  \n",
       "0 2022-09-19 22:46:35+00:00                1  \n",
       "1 2022-09-19 22:38:31+00:00                1  \n",
       "2 2022-09-19 21:55:37+00:00                4  \n",
       "3 2022-09-19 21:54:42+00:00                5  \n",
       "4 2022-09-19 21:53:51+00:00                5  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
